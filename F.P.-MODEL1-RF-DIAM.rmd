---
title: "F.P Model 1 RF DIAM"
author: "THEDION V. DIAM JR."
date: "2022-12-16"
output: pdf_document
---

# LOAD PACKAGES

```{r}
# Helper packages
library(readr)    # loading dataset
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(tidyverse)# for filtering 
library(rsample)  # for creating validation splits
library(bestNormalize) # for normalizing the dataset
library(stringr)       # for string functionality
library(gridExtra)     # for manipulaiting the grid

# Modeling packages
library(cluster)       # for general clustering algorithms
library(factoextra)    # for visualizing cluster results
library(ranger)     # a c++ implementation of random forest 
library(h2o)        # a java-based implementation of random forest
h2o.init()
```

# Load Data Sets
The data contains 197 rows and 431 columns with *Failure.binary* binary output.
```{r}
library(readr)
rawd <- read_csv("D:/DIAM/FP-DATA.csv")
```


#=================================================== Reprocessing the Raw Data ===================================================#
```{r}
library(tidyverse)
library(bestNormalize)
```

# Check for null and missing values
Using *anyNA()* function, We can determine if any missing values in our data.
```{r}
anyNA(rawd)

#The result shows either *True* or *False*. If True, omit the missing values using *na.omit()*
  
#[1] FALSE

#Thus, our data has no missing values.
```

# Check for Normality of the Data
We used *Shapiro-Wilk's Test* to check the normality of the data.

```{r,warning=F}
rd <- rawd%>%select_if(is.numeric) 
rd <- rd[,-1]
test <- apply(rd,2,function(x){shapiro.test(x)})
```


To have the list of p-value of all variables, the *unlist()* function is used and convert a list to vector.
```{r}
pvalue_list <- unlist(lapply(test, function(x) x$p.value))
```


```{r}
sum(pvalue_list<0.05)  # not normally distributed
sum(pvalue_list>0.05)  # normally distributed
test$Entropy_cooc.W.ADC

# [1] 428
# [1] 1

#  Thus, we have 428 variables that are not normally distributed and Entropy_cooc.W.ADC is normally distributed.
```

We use *orderNorm()* function, the *x.t*	is the elements of orderNorm() function transformed original data.Using the *Shapiro-Wilk's Test*
```{r,warning=F}
TRDrawd=rawd[,c(3,5:length(names(rawd)))]

TRDrawd=apply(TRDrawd,2,orderNorm)
TRDrawd=lapply(TRDrawd, function(x) x$x.t)
TRDrawd=TRDrawd%>%as.data.frame()
test=apply(TRDrawd,2,shapiro.test)
test=unlist(lapply(test, function(x) x$p.value))
```

#Testing Data 
```{r,warning=F}
sum(test <0.05)  # not normally distributed
sum(test >0.05)  # normally distributed

#[1] 0
#[1] 428

# Thus, our data is normally distributed.
```


```{r}
rawd[,c(3,5:length(names(rawd)))]=TRDrawd
```

Get the correlation of the whole data expect the categorical variables
```{r}
CorMatrix=cor(rawd[,-c(1,2)])
heatmap(CorMatrix,Rowv=NA,Colv=NA,scale="none",revC = T)
```

#Splitting the Data
Split the data into training (80%) and testing (20%).
```{r}
rawd$Institution=as.factor(rawd$Institution)
rawd$Failure.binary=as.factor(rawd$Failure.binary)
```

```{r}
splitter <- sample(1:nrow(rawd), round(nrow(rawd) * 0.8))
trainND <- rawd[splitter, ]
testND  <- rawd[-splitter, ]
```

The data frame output of data reprocessing will be converted into to "csv", which will be used for entire project.

# Load new Data 
```{r}
Final<-  read_csv("D:/DIAM/newdat.csv")
View(Final)

#=============RANDOM FOREST====================#

Random Forest builds and combines multiple decision trees to get more accurate predictions. Itâ€™s a non-linear classification algorithm

```{r}
# Helper packages
library(ROCR)
library(pROC)
# Modeling packages
library(ranger)   # a c++ implementation of random forest 
library(h2o)      # a java-based implementation of random forest
h2o.init()
```

# LOAD THE REPROCESSED DATASET

Note that we converted the reprocessed dataset into *csv*. Hence, this dataset we used for the entire project named *Newdat.CSV*

```{r}
# make bootstrapping reproducible

set.seed(123)  # for reproducibilityR
split <- initial_split(Final, strata = "Failure.binary")
traindt <- training(split)
testdt <- testing(split)


# number of features
no.features <- length(setdiff(names(traindt), "Failure.binary"))
```


```{r}
# train a default random forest model
randomforest1 <- ranger(
  Failure.binary ~ ., 
  data = traindt,
  mtry = floor(no.features / 3),
  respect.unordered.factors = "order",
  seed = 123
)
```


```{r}
# get OOB RMSE
(default_rmse <- sqrt(randomforest1$prediction.error))

# create hyperparameter grid
hyper_grid <- expand.grid(
  mtry = floor(no.features * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)

# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = Failure.binary ~ ., 
    data            = traindt, 
    num.trees       = no.features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
```


```{r}
# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)

h2o.no_progress()
h2o.init(max_mem_size = "5g")
```


```{r}
# converting training data to h2o object
train_h2o <- as.h2o(traindt)

# set the response column to Failure.binary
response <- "Failure.binary"

# set the predictor names
predictors <- setdiff(colnames(traindt), response)

h2o_rf1 <- h2o.randomForest(
  x = predictors, 
  y = response,
  training_frame = train_h2o, 
  ntrees = no.features * 10,
  seed = 123
)

h2o_rf1
```


```{r}
# hyperparameter grid
hyper_grid <- list(
  mtries = floor(no.features * c(.05, .15, .25, .333, .4)),
  min_rows = c(1, 3, 5, 10),
  max_depth = c(10, 20, 30),
  sample_rate = c(.55, .632, .70, .80)
)

# random grid search strategy
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,   # stop if improvement is < 0.1%
  stopping_rounds = 10,         # over the last 10 models
  max_runtime_secs = 60*5      # or stop search after 5 min.
)
```


```{r}
# perform grid search 
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response, 
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  ntrees = no.features * 10,
  seed = 123,
  stopping_metric = "RMSE",   
  stopping_rounds = 10,           # stop if last 10 trees added 
  stopping_tolerance = 0.005,     # don't improve RMSE by 0.5%
  search_criteria = search_criteria
)
```


```{r}
# collect the results and sort by our model performance metric 
# of choice
random_grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)
random_grid_perf
```


```{r}
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = Failure.binary ~ ., 
  data = traindt, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```


```{r}
# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = Failure.binary ~ ., 
  data = traindt, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```


```{r}
p1 <- vip::vip(rf_impurity, num_features = 25, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 25, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)


```

```{r}
# Compute predicted probabilities on training data
m1_prob <- predict(h2o_rf1, train_h2o, type = "prob")
m1_prob=as.data.frame(m1_prob)[,2]
train_h2o=as.data.frame(train_h2o)
# Compute AUC metrics for cv_model1,2 and 3 
perf1 <- prediction(m1_prob,train_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")


# Plot ROC curves for cv_model1,2 and 3 
plot(perf1, col = "black", lty = 2)


# ROC plot for training data
roc( train_h2o$Failure.binary ~ m1_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)



# #Feature Interpretation
# vip(cv_model3, num_features = 20)

# Compute predicted probabilities on training data
test_h2o=as.h2o(testdt)

m2_prob <- predict(h2o_rf1, test_h2o, type = "prob")

m2_prob=as.data.frame(m2_prob)[,2]

test_h2o=as.data.frame(test_h2o)

# Compute AUC metrics for cv_model1,2 and 3 
perf2 <- prediction(m2_prob,test_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")


# Plot ROC curves for cv_model1,2 and 3 
plot(perf2, col = "black", lty = 2)


# ROC plot for training data
roc( test_h2o$Failure.binary ~ m2_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)

```
